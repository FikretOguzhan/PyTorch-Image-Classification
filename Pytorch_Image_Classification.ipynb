{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFSxFAMF+cqhTZmEPT1PNl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "PyTorch is an open-source machine learning library for Python that provides a flexible and dynamic computational graph. It is widely used for deep learning and artificial intelligence research and development. It is one of the latest deep learning frameworks and was developed by the team at Facebook and open sourced on GitHub in 2017.\n",
        "\n",
        "#### Why we use PyTorch\n",
        "Simplicity and Ease of Use: PyTorch is known for its simplicity and ease of use, which makes it a popular choice among developers.\n",
        "Dynamic Computational Graph: PyTorch uses dynamic computational graphs, which allows for flexibility in building and modifying models on the fly, which is beneficial for complex architectures.\n",
        "Pythonic and Intuitive Syntax: PyTorch adopts a Pythonic programming style, making it easy to understand and use. Its syntax closely resembles standard Python code, which reduces the learning curve and simplifies rapid development.\n",
        "#### Difference Between PyTorch and TensorFlow\n",
        "PyTorch and TensorFlow are the two most popular deep learning libraries. They are both open source and have strong communities. However it can be said that they differ significantly at 2 different points.\n",
        "\n",
        " - Computational Graph: Pytorch uses a dynamic computational graph which means that the graph is built on the fly as operations are executed. This can make it more intuitive for debugging and experimentation. However TensorFlow uses a static computational graph. his means that the structure of the computational graph in TensorFlow is fixed when the graph is defined, whereas in PyTorch, the graph can be changed dynamically as the program runs.\n",
        " - API: Both PyTorch and TensorFlow offer useful abstractions that ease the development of models by reducing boilerplate code. However, they differ in their design philosophy, syntax, and features. PyTorch has a more “pythonic” approach and is object-oriented, while TensorFlow offers a variety of options. TensorFlow offers better visualization, which allows developers to debug better and track the training process. PyTorch, however, provides only limited visualization. TensorFlow excels in deploying trained models to production, thanks to the TensorFlow Serving framework. PyTorch does not offer such a framework, so developers need to use Django or Flask as a back-end server.\n",
        "In conclusion, the choice between PyTorch and TensorFlow depends on the specific needs of your project. If you require a more pythonic approach and prefer a dynamic computational graph, PyTorch might be a better choice. If you need better visualization tools, production-ready deployment options, or a larger community and ecosystem, TensorFlow might be more suitable.\n"
      ],
      "metadata": {
        "id": "ZqMsrbmW-OEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCAUS_M-czib"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "!unrar x \"/content/drive/MyDrive/raw_data (1).rar\"\n",
        "\n",
        "raw_data_dir = \"/content/raw_data\"\n",
        "print(os.listdir(raw_data_dir))\n",
        "\n",
        "categories = os.listdir(raw_data_dir)\n",
        "num_of_categories = len(categories)\n",
        "print(\"Counts of Images = {}\".format(num_of_categories))\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"premolar\")\n",
        "os.mkdir(\"molar\")\n",
        "os.mkdir(\"incisor\")\n",
        "os.mkdir(\"canine\")\n",
        "\n",
        "for i in categories:\n",
        "  if \"premolar\" in i:\n",
        "    shutil.move(\"/content/raw_data/\" + i, \"/content/premolar\")\n",
        "  elif \"molar\" in i:\n",
        "    shutil.move(\"/content/raw_data/\" + i, \"/content/molar\")\n",
        "  elif \"incisor\" in i:\n",
        "    shutil.move(\"/content/raw_data/\" + i, \"/content/incisor\")\n",
        "  elif \"canine\" in i:\n",
        "    shutil.move(\"/content/raw_data/\" + i, \"/content/canine\")\n",
        "\n",
        "print(\"Count of Premolar: {}\".format(len(os.listdir(\"premolar\"))))\n",
        "print(\"Count of Molar: {}\".format(len(os.listdir(\"molar\"))))\n",
        "print(\"Count of Incisor: {}\".format(len(os.listdir(\"incisor\"))))\n",
        "print(\"Count of Canine: {}\".format(len(os.listdir(\"canine\"))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step by Step Coding\n",
        "The dataset we will use will consist of 4 different classes. These are canine, molar, premolar and incisor respectively. Our expectation from the model we will create is to classify these teeth correctly. There are a total of 10876 images in our dataset. We will allocate 7613 of them for train. the rest will be divided into two and we will create the test and validation parts.\n",
        "\n",
        "Importing the libraries we will use:"
      ],
      "metadata": {
        "id": "OXzr7cP6-nk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "from random import shuffle\n",
        "import torch.nn.functional as F\n",
        "import tqdm.notebook as tqdm\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "_oCQNZBhdH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/data\"\n",
        "categories = os.listdir(data_dir)\n",
        "num_of_categories = len(categories)\n",
        "print(num_of_categories)\n",
        "\n",
        "image_count_graph = pd.DataFrame(\n",
        "    index = [i for i in categories],\n",
        "    data = [len(os.listdir(data_dir + \"/\" + name))\n",
        "              for name in categories],\n",
        "    columns = [\"number_of_images\"]\n",
        ")\n",
        "image_count_graph\n",
        "\n",
        "\n",
        "fig = image_count_graph.plot(kind = \"bar\", figsize = (12,8))\n",
        "mean_of_pictures = image_count_graph[\"number_of_images\"].mean()\n",
        "fig.axhline(mean_of_pictures, color = \"r\", linestyle = \":\", label = \"Mean of Images\")\n",
        "plt.title(\"Image Count by Class\", fontsize = 20)\n",
        "plt.show()\n",
        "print(\"Mean Value of Image Numbers :\", mean_of_pictures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "h51T7ryJdxeQ",
        "outputId": "830718ef-b578-4e51-d528-d5b678b838f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Value of Image Numbers : 2719.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if CUDA(Compute Unified Device Architecture) is available for use with PyTorch. CUDA is a parallel computing platform and API model created by NVIDIA. It allows software developers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing."
      ],
      "metadata": {
        "id": "a6Hmv8QE-xAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device=torch.device(\"cuda:0\")\n",
        "    print(\"Training on GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Training on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNxDXv-hd0MC",
        "outputId": "8483027a-26b2-4a79-a76d-de10a4730e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it’s time for data transform. Here we will add more than one transformation. These are RandomResizedCrop, RandomRotation, ColorJitter, RandomHorizontalFlip, CenterCrop, ToTensor and Normalize. The reason we chose size 224 in CentreCrop is that we want it to comply with ImageNet standards. Likewise, Normalise values have been created accordingly. However, you can change the other transforms as you wish and observe the effect on the model success."
      ],
      "metadata": {
        "id": "b5fsqak3-0AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = {\n",
        "    \"train\":\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size = 256, scale = (0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees = 15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size = 224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"test\":\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size = 256),\n",
        "        transforms.CenterCrop(size = 224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "SfFdZRIjd2pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to split image dataset training, validation, and testing sets, apply transformations to those sets, and then load them into PyTorch DataLoaders for use in training a model. Each stage is described with a comment line."
      ],
      "metadata": {
        "id": "IiIQtRgP-7Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading data from the directory and applying the transform.\n",
        "data = datasets.ImageFolder(root = \"/content/data\", transform = img_transforms[\"train\"])\n",
        "\n",
        "#Calculating the lengths of the training, validation and test sets.\n",
        "#Splitting the dataset into 70% training, 15% validation, and 15% test data.\n",
        "train_data_len = int(len(data)*0.7)\n",
        "valid_data_len = int((len(data) - train_data_len)/2)\n",
        "test_data_len = int(len(data) - train_data_len - valid_data_len)\n",
        "\n",
        "print(\"Train Data Length :\", train_data_len)\n",
        "print(\"Validation Data Length :\", valid_data_len)\n",
        "print(\"Test Data Length :\", test_data_len)\n",
        "\n",
        "#Creating train, validation and test data randomly.\n",
        "train_data, val_data, test_data = random_split(data, [train_data_len, valid_data_len, test_data_len])\n",
        "\n",
        "#Applying the specified transformations to the training, validation, and test datasets.\n",
        "train_data.dataset.transform = img_transforms['train']\n",
        "val_data.dataset.transform = img_transforms['test']\n",
        "test_data.dataset.transform = img_transforms['test']\n",
        "\n",
        "print(\"Length of train data: \", len(train_data), \"\\n\",\n",
        "      \"Length of val data:\",  len(val_data), \"\\n\",\n",
        "      \"Length of test data:\", len(test_data))\n",
        "\n",
        "#Creating data loaders for the training, validation, and test datasets. The DataLoader class from torch.utils.data is used to load the data in batches of size batch, and the data is shuffled for each epoch during training.\n",
        "batch = 32\n",
        "train_loader = DataLoader(dataset = train_data, batch_size=batch, shuffle=True)\n",
        "val_loader = DataLoader(dataset = val_data, batch_size=batch, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=batch, shuffle=True)\n",
        "\n",
        "#Creating dictionaries for the data loaders and the sizes of the training and validation datasets.\n",
        "dataloaders = {\"train\":train_loader, \"val\":val_loader}\n",
        "data_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
        "\n",
        "#An iterator for the training data loader and get the first batch of features and labels.\n",
        "trainiter = iter(train_loader)\n",
        "features, labels = next(trainiter)\n",
        "\n",
        "print(features.shape, labels.shape)\n",
        "print(data_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncbLXx9fd5CA",
        "outputId": "f03f0fcd-655a-484b-be8e-a5e345aa98b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Length : 7613\n",
            "Validation Data Length : 1631\n",
            "Test Data Length : 1632\n",
            "Length of train data:  7613 \n",
            " Length of val data: 1631 \n",
            " Length of test data: 1632\n",
            "torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
            "{'train': 7613, 'val': 1631}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic CNN\n",
        "First, we will use a CNN model that we will create from scratch in the classification task and analyse the results. In the comment lines of the code, the new shape after each operation is explained. In addition, let’s look at what we do in each layer one by one.\n",
        "\n",
        " - Convolutional Layer 1 (conv1): This layer takes an input with 3 channels (corresponding to the RGB color channels of the input image) and applies a convolution operation with 12 filters, each of size 3x3. The stride is 1 and padding is 1, which means the convolution operation will be applied to all pixels in the input, and the output will have the same width and height as the input.\n",
        " - Batch Normalization Layer 1 (bn1): This layer normalizes the output from the first convolutional layer. It helps in improving the performance of the network by reducing the internal covariate shift, which is a change in the distribution of layer inputs.\n",
        " - ReLU Activation Layer 1 (relu1): This layer applies the Rectified Linear Unit (ReLU) activation function to the output from the batch normalization layer. The ReLU function helps to introduce non-linearity into the network.\n",
        " - Max Pooling Layer (pool): This layer reduces the spatial dimensions (width and height) of the input by taking the maximum value over a 2x2 window. This helps to reduce the number of parameters in the network and control overfitting.\n",
        " - Convolutional Layer 2 (conv2): This layer applies a convolution operation with 20 filters, each of size 3x3, to the output from the max pooling layer. The stride is 1 and padding is 1.\n",
        " - ReLU Activation Layer 2 (relu2): This layer applies the ReLU activation function to the output from the second convolutional layer.\n",
        " - Convolutional Layer 3 (conv3): This layer applies a convolution operation with 32 filters, each of size 3x3, to the output from the ReLU activation layer of the second convolutional layer.\n",
        " - Batch Normalization Layer 3 (bn3): This layer normalizes the output from the third convolutional layer.\n",
        " - ReLU Activation Layer 3 (relu3): This layer applies the ReLU activation function to the output from the batch normalization layer of the third convolutional layer.\n",
        " - Fully Connected Layer 1 (fc1): This layer flattens the output from the ReLU activation layer of the third convolutional layer into a 1D tensor and applies a linear transformation. It has 120 output features.\n",
        " - Fully Connected Layer 2 (fc2): This layer applies a linear transformation to the output from the first fully connected layer. The number of output features is equal to the number of classes in the classification task.\n",
        "The forward function defines the sequence in which these layers are applied to the input. The output of each layer is passed as the input to the next layer. The output of the second fully connected layer is the final output of the network."
      ],
      "metadata": {
        "id": "m3cK13J__XuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, num_classes = 4):\n",
        "    super(ConvNet, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 12, kernel_size=3, stride = 1, padding = 1)\n",
        "    #Shape = (32, 12, 224, 224)\n",
        "    self.bn1 = nn.BatchNorm2d(num_features = 12)\n",
        "    #Shape = (32, 12, 224, 224)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    #Shape = (32, 12, 224, 224)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2)\n",
        "    #Reduce the image size be factor 2\n",
        "    #Shape = (32, 12, 112, 112)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 12, out_channels = 20, kernel_size = 3, stride = 1, padding = 1)\n",
        "    #Shape = (32, 20, 112, 112)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    #Shape = (32, 20, 112, 112)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 20, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(num_features=32)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    #Shape = (32, 32, 112, 112)\n",
        "    self.fc1 = nn.Linear(in_features = 112*112*32, out_features = 120)\n",
        "    self.fc2 = nn.Linear(in_features = 120, out_features = num_classes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.conv1(input)\n",
        "    output = self.bn1(output)\n",
        "    output = self.relu1(output)\n",
        "    output = self.pool(output)\n",
        "    output = self.conv2(output)\n",
        "    output = self.relu2(output)\n",
        "    output = self.conv3(output)\n",
        "    output = self.bn3(output)\n",
        "    output = self.relu3(output)\n",
        "    #Above output will be in matrix form, with shape (32,32,112, 112)\n",
        "    output = output.view(-1, 112*112*32)\n",
        "    output = F.relu(self.fc1(output))\n",
        "    output = self.fc2(output)\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "_AOdHsCtd7RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s define the model and create the hyperparameters. These created hyperparameters will be exactly the same in the models we will use in the future."
      ],
      "metadata": {
        "id": "aLA_ZVff_y8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet(num_classes = 4).to(device)\n",
        "#Optmizer and loss function\n",
        "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
        "loss_function=nn.CrossEntropyLoss()\n",
        "num_epochs = 8"
      ],
      "metadata": {
        "id": "W2EZoLOo_yUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = train_data_len\n",
        "test_count = test_data_len\n",
        "\n",
        "print(train_count, test_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_saDlkOeVxH",
        "outputId": "b8b86435-aee3-478e-cfb0-08d1af59669b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7613 1632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training and saving the best model"
      ],
      "metadata": {
        "id": "2BQqcPNo_3K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.cpu().data * images.size(0)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy / train_count\n",
        "    train_loss = train_loss / train_count\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    test_accuracy = test_accuracy / test_count\n",
        "\n",
        "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
        "          ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "    # Save the best model\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
        "        best_accuracy = test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIm4vP04emo3",
        "outputId": "366e2a4a-7899-4c17-b200-293d26e8fa3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(4.7534) Train Accuracy: 0.44489688690398005 Test Accuracy: 0.4944852941176471\n",
            "Epoch: 1 Train Loss: tensor(1.0115) Train Accuracy: 0.5326415342177854 Test Accuracy: 0.5116421568627451\n",
            "Epoch: 2 Train Loss: tensor(0.9451) Train Accuracy: 0.5640352029423354 Test Accuracy: 0.5306372549019608\n",
            "Epoch: 3 Train Loss: tensor(0.9299) Train Accuracy: 0.5883357414948115 Test Accuracy: 0.59375\n",
            "Epoch: 4 Train Loss: tensor(0.9170) Train Accuracy: 0.5970051228162354 Test Accuracy: 0.48713235294117646\n",
            "Epoch: 5 Train Loss: tensor(0.8904) Train Accuracy: 0.608038880861684 Test Accuracy: 0.5526960784313726\n",
            "Epoch: 6 Train Loss: tensor(0.8323) Train Accuracy: 0.6341783790884015 Test Accuracy: 0.6170343137254902\n",
            "Epoch: 7 Train Loss: tensor(0.8094) Train Accuracy: 0.6582162091159858 Test Accuracy: 0.6200980392156863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model we created obtained 0.62 as the highest validation accuracy after 8 epochs. Not a bad result considering that it is a very basic model. Now let’s try the well-known and frequently used Xception model both with and without transfer learning and see the results."
      ],
      "metadata": {
        "id": "HbR0XcDaAJUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xception with Transfer Learning"
      ],
      "metadata": {
        "id": "lUTt1ZMq7HFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import timm\n",
        "\n",
        "# Assuming train_loader and test_loader are defined\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model and move it to the appropriate device\n",
        "model = timm.create_model('xception', pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model training and saving best model\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.cpu().data * images.size(0)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy / train_count\n",
        "    train_loss = train_loss / train_count\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    test_accuracy = test_accuracy / test_count\n",
        "\n",
        "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
        "          ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "    # Save the best model\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
        "        best_accuracy = test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-FsTtj1htUP",
        "outputId": "362f14bf-18a6-4ad4-835d-fb0ddc6d60ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.11-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth\" to /root/.cache/torch/hub/checkpoints/xception-43020ad28.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(0.7809) Train Accuracy: 0.7271771969000395 Test Accuracy: 0.8075980392156863\n",
            "Epoch: 1 Train Loss: tensor(0.4320) Train Accuracy: 0.836201234730067 Test Accuracy: 0.8327205882352942\n",
            "Epoch: 2 Train Loss: tensor(0.3314) Train Accuracy: 0.8787600157625115 Test Accuracy: 0.8382352941176471\n",
            "Epoch: 3 Train Loss: tensor(0.2262) Train Accuracy: 0.918429003021148 Test Accuracy: 0.8363970588235294\n",
            "Epoch: 4 Train Loss: tensor(0.1818) Train Accuracy: 0.9345855773019834 Test Accuracy: 0.8694852941176471\n",
            "Epoch: 5 Train Loss: tensor(0.1617) Train Accuracy: 0.9399711020622619 Test Accuracy: 0.8498774509803921\n",
            "Epoch: 6 Train Loss: tensor(0.1182) Train Accuracy: 0.9612504925784842 Test Accuracy: 0.8462009803921569\n",
            "Epoch: 7 Train Loss: tensor(0.1193) Train Accuracy: 0.956390384867989 Test Accuracy: 0.8449754901960784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By saying pretrained = True, we also include transfer learning. Again, we use the same hyperparameters and train the model using the same code we wrote for training above. The highest validation accuracy for this model is 0.87. Now let’s try it without transfer learning."
      ],
      "metadata": {
        "id": "xLlE7Ih4ARE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xception without Transfer Learning"
      ],
      "metadata": {
        "id": "9oOj54Si7RPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install timm\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import timm\n",
        "\n",
        "# Assuming train_loader and test_loader are defined\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model and move it to the appropriate device\n",
        "model = timm.create_model('xception', pretrained=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model training and saving best model\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.cpu().data * images.size(0)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy / train_count\n",
        "    train_loss = train_loss / train_count\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    test_accuracy = test_accuracy / test_count\n",
        "\n",
        "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
        "          ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "    # Save the best model\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
        "        best_accuracy = test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZhO29Jh4SE",
        "outputId": "ad611a90-9c97-4e6b-c8ca-2b44e7a388c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.10)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Epoch: 0 Train Loss: tensor(1.0723) Train Accuracy: 0.5624589517929857 Test Accuracy: 0.6421568627450981\n",
            "Epoch: 1 Train Loss: tensor(0.8644) Train Accuracy: 0.6326021279390516 Test Accuracy: 0.6580882352941176\n",
            "Epoch: 2 Train Loss: tensor(0.7846) Train Accuracy: 0.6689872586365427 Test Accuracy: 0.6868872549019608\n",
            "Epoch: 3 Train Loss: tensor(0.7093) Train Accuracy: 0.7083935373702877 Test Accuracy: 0.7340686274509803\n",
            "Epoch: 4 Train Loss: tensor(0.6504) Train Accuracy: 0.7351898069092342 Test Accuracy: 0.7555147058823529\n",
            "Epoch: 5 Train Loss: tensor(0.5793) Train Accuracy: 0.7626428477604098 Test Accuracy: 0.7763480392156863\n",
            "Epoch: 6 Train Loss: tensor(0.5243) Train Accuracy: 0.7952187048469723 Test Accuracy: 0.7591911764705882\n",
            "Epoch: 7 Train Loss: tensor(0.4592) Train Accuracy: 0.8184684092998817 Test Accuracy: 0.772671568627451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we can clearly see the effect of transfer learning. The highest validation accuracy dropped from 0.87 to 0.77. Let’s apply the same thing we applied to Xception for ResNet50 and take a look at the results."
      ],
      "metadata": {
        "id": "povqHQPFBBY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50 with Transfer Learning"
      ],
      "metadata": {
        "id": "u7ZsWyZ9DlzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained = True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model training and saving best model\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.cpu().data * images.size(0)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy / train_count\n",
        "    train_loss = train_loss / train_count\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    test_accuracy = test_accuracy / test_count\n",
        "\n",
        "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
        "          ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "    # Save the best model\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
        "        best_accuracy = test_accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJICdtAwCGkP",
        "outputId": "693c1d10-807e-4df5-fe4c-fec35455efc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 128MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(1.0100) Train Accuracy: 0.6097464862734796 Test Accuracy: 0.6158088235294118\n",
            "Epoch: 1 Train Loss: tensor(0.7474) Train Accuracy: 0.693813214238802 Test Accuracy: 0.6850490196078431\n",
            "Epoch: 2 Train Loss: tensor(0.6487) Train Accuracy: 0.7370287665834756 Test Accuracy: 0.7800245098039216\n",
            "Epoch: 3 Train Loss: tensor(0.5821) Train Accuracy: 0.7730198344936293 Test Accuracy: 0.7610294117647058\n",
            "Epoch: 4 Train Loss: tensor(0.5257) Train Accuracy: 0.7977144358334428 Test Accuracy: 0.7953431372549019\n",
            "Epoch: 5 Train Loss: tensor(0.4701) Train Accuracy: 0.8204387232365691 Test Accuracy: 0.7824754901960784\n",
            "Epoch: 6 Train Loss: tensor(0.4413) Train Accuracy: 0.8314724812820176 Test Accuracy: 0.7996323529411765\n",
            "Epoch: 7 Train Loss: tensor(0.3764) Train Accuracy: 0.8615526073821096 Test Accuracy: 0.8345588235294118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest validation accuracy obtained using the same hyperparameters was 0.83. The model performed lower than Xception with transfer learning. Finally, let’s train without transfer learning and look at the results."
      ],
      "metadata": {
        "id": "qBshchz2BfjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained = False)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model training and saving best model\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.cpu().data * images.size(0)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy / train_count\n",
        "    train_loss = train_loss / train_count\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = Variable(images.to(device))\n",
        "        labels = Variable(labels.to(device))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    test_accuracy = test_accuracy / test_count\n",
        "\n",
        "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
        "          ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "    # Save the best model\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
        "        best_accuracy = test_accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhfYdpLDkyS",
        "outputId": "4ce2d18f-0889-4099-8760-580c57285be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(1.1364) Train Accuracy: 0.5280441350321818 Test Accuracy: 0.5729166666666666\n",
            "Epoch: 1 Train Loss: tensor(0.9858) Train Accuracy: 0.5804544857480626 Test Accuracy: 0.4497549019607843\n",
            "Epoch: 2 Train Loss: tensor(0.9554) Train Accuracy: 0.5909628267437278 Test Accuracy: 0.5447303921568627\n",
            "Epoch: 3 Train Loss: tensor(0.9006) Train Accuracy: 0.6213056613687115 Test Accuracy: 0.539828431372549\n",
            "Epoch: 4 Train Loss: tensor(0.8761) Train Accuracy: 0.630894522527256 Test Accuracy: 0.6550245098039216\n",
            "Epoch: 5 Train Loss: tensor(0.8488) Train Accuracy: 0.6457375541836332 Test Accuracy: 0.6139705882352942\n",
            "Epoch: 6 Train Loss: tensor(0.7898) Train Accuracy: 0.6655720478129515 Test Accuracy: 0.5949754901960784\n",
            "Epoch: 7 Train Loss: tensor(0.7513) Train Accuracy: 0.6789701825824248 Test Accuracy: 0.6887254901960784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 performed very badly without Transfer Learning. The highest validation accuracy obtained was 0.68."
      ],
      "metadata": {
        "id": "pt95ksVOBlN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that Xception and Resnet models show a very successful performance when transfer learning is applied. Without any fine tuning or any work to increase performance, it is a result that can be called quite sufficient that both of them see above 85%. In addition, it is also noteworthy that the basic cnn model is successful than the transfer learning resnet model. The reasons for this can be listed as follows:\n",
        "\n",
        " - Dataset Suitability: Large and complex models such as ResNet50 usually perform better on a large and diverse dataset. However, sometimes a simple model can give better results on a smaller dataset.\n",
        " - Hyperparameters: The best model architecture for each problem may differ, so building a specific model for a specific problem can sometimes give better results.\n",
        " - Overfitting: Complex models, such as ResNet-50, usually include more parameters, which can increase the risk of overfitting. The simple model can have fewer parameters, which can reduce the risk of overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "8IbujcMYBo5A"
      }
    }
  ]
}